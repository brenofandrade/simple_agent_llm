# Chat Agent API - Arquitetura Refatorada

Sistema de chat inteligente com roteamento automÃ¡tico e busca hÃ­brida no Pinecone.

## ğŸ—ï¸ Arquitetura

### Fluxo de Processamento

```
UsuÃ¡rio
  â†“
[Flask API - Porta 8000]
  â†“
[Router - Classifica pergunta via OpenAI]
  â†“
â”œâ”€â†’ greetings          â†’ Resposta direta
â”œâ”€â†’ farewell           â†’ Resposta direta
â”œâ”€â†’ clarification      â†’ Pede mais detalhes
â”œâ”€â†’ general_knowledge  â†’ Usa LLM diretamente
â””â”€â†’ internal_docs      â†’ Agente usa Pinecone Tool
                           â†“
                      [Busca HÃ­brida]
                      - Similaridade vetorial
                      - Reranking com cross-encoder
                           â†“
                      [Resposta com contexto]
```

### Componentes Principais

#### 1. **router.py** - Classificador de Perguntas
- Usa OpenAI GPT-4o-mini
- Classifica em 5 categorias:
  - `greetings` - SaudaÃ§Ãµes
  - `farewell` - Despedidas
  - `clarification_needed` - Perguntas vagas
  - `internal_docs` - Documentos internos
  - `general_knowledge` - Conhecimento geral

#### 2. **tools.py** - Pinecone Search Tool
- **Busca HÃ­brida**:
  - Gera variaÃ§Ãµes da query
  - Busca por similaridade vetorial
  - Reranking com cross-encoder (ms-marco-MiniLM-L-6-v2)
- **Embeddings**: OpenAI ou Ollama (configurÃ¡vel)
- **DeduplicaÃ§Ã£o** automÃ¡tica de resultados

#### 3. **agent.py** - Agente Inteligente
- Recebe classificaÃ§Ã£o do router
- **Decide autonomamente** quando usar Pinecone tool
- Para `internal_docs`: sempre busca contexto
- Gera respostas contextualizadas

#### 4. **main.py** - Flask API
- Porta: **8000**
- Endpoints:
  - `POST /chat` - Processa perguntas
  - `POST /clear` - Limpa memÃ³ria
  - `GET /health` - Health check
  - `GET /` - Info da API

## ğŸš€ Como Usar

### 1. Configurar Ambiente

Crie um arquivo `.env`:

```bash
# OpenAI
OPENAI_API_KEY=sk-...

# Pinecone
PINECONE_API_KEY_DSUNIBLU=your-pinecone-key
PINECONE_INDEX=your-index-name
PINECONE_NAMESPACE=default

# Ollama (opcional)
OLLAMA_BASE_URL=http://localhost:11434
GENERATION_MODEL=llama3.2:latest
EMBEDDING_MODEL=mxbai-embed-large:latest

# Flask
BACKEND_PORT=8000
FLASK_DEBUG=0

# Outros
LOG_LEVEL=INFO
MAX_HISTORY=10
RETRIEVAL_K=5
```

### 2. Instalar DependÃªncias

```bash
pip install -r requirements.txt
```

### 3. Iniciar AplicaÃ§Ã£o

```bash
python main.py
```

A API estarÃ¡ disponÃ­vel em: `http://localhost:8000`

## ğŸ“¡ Exemplos de Uso

### 1. SaudaÃ§Ã£o

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -H "X-Session-Id: user123" \
  -d '{
    "question": "OlÃ¡!"
  }'
```

**Resposta:**
```json
{
  "question": "OlÃ¡!",
  "answer": "OlÃ¡! Como posso ajudÃ¡-lo hoje?",
  "question_type": "greetings",
  "used_tool": false,
  "sources": null,
  "latency_ms": 234.56
}
```

### 2. Pergunta sobre Documentos Internos

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -H "X-Session-Id: user123" \
  -d '{
    "question": "Qual o procedimento de reembolso para viagens?",
    "k": 5,
    "namespace": "default"
  }'
```

**Resposta:**
```json
{
  "question": "Qual o procedimento de reembolso para viagens?",
  "answer": "De acordo com o Manual de PolÃ­ticas...",
  "question_type": "internal_docs",
  "used_tool": true,
  "sources": [
    {
      "rank": 1,
      "source": "Manual de PolÃ­ticas (pÃ¡gina 15)",
      "score": 0.89,
      "rerank_score": 0.95,
      "content_preview": "Para reembolso de viagens...",
      "metadata": {...}
    }
  ],
  "tool_info": {
    "tool_name": "pinecone_search",
    "num_docs_found": 3,
    "k": 5,
    "namespace": "default"
  },
  "latency_ms": 1523.45
}
```

### 3. Pergunta de Conhecimento Geral

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -H "X-Session-Id: user123" \
  -d '{
    "question": "O que Ã© machine learning?"
  }'
```

**Resposta:**
```json
{
  "question": "O que Ã© machine learning?",
  "answer": "Machine Learning Ã© um subcampo da inteligÃªncia artificial...",
  "question_type": "general_knowledge",
  "used_tool": false,
  "sources": null,
  "latency_ms": 567.89
}
```

### 4. Limpar MemÃ³ria

```bash
curl -X POST http://localhost:8000/clear \
  -H "Content-Type: application/json" \
  -H "X-Session-Id: user123"
```

## ğŸ”§ ParÃ¢metros da API

### POST /chat

**Headers:**
- `X-Session-Id` (obrigatÃ³rio): ID da sessÃ£o do usuÃ¡rio

**Body (JSON):**
- `question` (obrigatÃ³rio): Pergunta do usuÃ¡rio
- `session_id` (opcional): Alternativa ao header
- `k` (opcional, padrÃ£o=5): NÃºmero de documentos a buscar
- `namespace` (opcional): Namespace do Pinecone

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Trocar Modelo de GeraÃ§Ã£o

No `main.py:89-92`, altere:

```python
IntelligentAgent(
    session_id=session_id,
    use_openai_for_generation=True  # True = OpenAI, False = Ollama
)
```

### Ajustar Reranking

No `agent.py:282-288`, altere o mÃ©todo:

```python
search_results = self.pinecone_tool.search(
    query=question,
    k=k,
    namespace=namespace,
    rerank_method="cross-encoder",  # "none", "embedding", "cross-encoder"
    rerank_top_k=3
)
```

## ğŸ†• MudanÃ§as Principais

### âŒ Removido
- Flag `use_rag` - O agente decide automaticamente
- CÃ³digo duplicado de prompts
- ConversationBufferMemory (conflitos de pacotes)

### âœ… Adicionado
- Arquitetura baseada em Tools
- Busca hÃ­brida no Pinecone
- Reranking com cross-encoder
- Cache de agentes por sessÃ£o
- ClassificaÃ§Ã£o automÃ¡tica com OpenAI

## ğŸ“Š Estrutura de Arquivos

```
simple_agent_llm/
â”œâ”€â”€ main.py              # Flask API (porta 8000)
â”œâ”€â”€ agent.py             # Agente inteligente
â”œâ”€â”€ router.py            # Classificador de perguntas
â”œâ”€â”€ tools.py             # Pinecone search tool
â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes
â”œâ”€â”€ requirements.txt     # DependÃªncias
â””â”€â”€ .env                 # VariÃ¡veis de ambiente
```

## ğŸ› Debug

Para ativar logs detalhados:

```bash
export LOG_LEVEL=DEBUG
python main.py
```

## ğŸ“ Notas

1. **Pinecone obrigatÃ³rio**: Para `internal_docs`, o Pinecone deve estar configurado
2. **OpenAI obrigatÃ³ria**: O router usa GPT-4o-mini para classificaÃ§Ã£o
3. **Ollama opcional**: Pode usar para geraÃ§Ã£o de respostas (economia de custos)
4. **SessÃµes**: Cada sessÃ£o mantÃ©m histÃ³rico independente (max 10 mensagens)
5. **TTL**: SessÃµes expiram apÃ³s 1200s (20 minutos) de inatividade

## ğŸ¯ PrÃ³ximos Passos

- [ ] Adicionar telemetria e mÃ©tricas
- [ ] Implementar cache de embeddings
- [ ] Adicionar suporte a mÃºltiplos namespaces
- [ ] Interface web para testes
- [ ] Testes automatizados
